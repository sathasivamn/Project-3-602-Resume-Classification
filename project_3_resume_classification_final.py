# -*- coding: utf-8 -*-
"""Project 3_Resume Classification Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dfp5SsjHSXLTqM8iImkaCH9bItcy5M5t
"""

# Install libraries
!pip install python-docx PyPDF2 nltk joblib scikit-learn matplotlib pandas seaborn streamlit
!apt-get update -y
!apt-get install -y libreoffice  # for .doc → .docx (optional if you have .doc files)

# Imports and NLTK setup
import os
import zipfile
from pathlib import Path

import pandas as pd
import numpy as np

from docx import Document
import PyPDF2

import nltk
from nltk.corpus import stopwords
import re

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report, confusion_matrix
)

import matplotlib.pyplot as plt
import seaborn as sns

import joblib

#Download NLTK data:
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))

"""# Upload and extract dataset zip

"""

from google.colab import files

uploaded = files.upload()  # upload resume_classification.zip

ZIP_NAME = "resume_classification.zip"   # change if your zip has a different name
EXTRACT_DIR = "/content/data"
os.makedirs(EXTRACT_DIR, exist_ok=True)

if ZIP_NAME in uploaded:
    with zipfile.ZipFile(ZIP_NAME, "r") as z:
        z.extractall(EXTRACT_DIR)
    print("Zip extracted to:", EXTRACT_DIR)
else:
    raise FileNotFoundError("Upload your dataset zip using files.upload()")

# (Optional) Convert .doc → .docx with LibreOffice
base = Path(EXTRACT_DIR)

doc_files = list(base.rglob("*.doc"))
print("Found .doc files:", len(doc_files))

for doc in doc_files:
    outdir = str(doc.parent)
    cmd = f'soffice --headless --convert-to docx "{doc}" --outdir "{outdir}"'
    os.system(cmd)

print("DOC → DOCX conversion completed.")

# Define file readers (DOCX + PDF)
def read_docx(path):
    try:
        d = Document(path)
        return "\n".join(p.text for p in d.paragraphs)
    except Exception as e:
        # print("DOCX read error:", e)
        return ""

def read_pdf(path):
    try:
        r = PyPDF2.PdfReader(path)
        all_text = ""
        for p in r.pages:
            all_text += p.extract_text() or ""
        return all_text
    except Exception as e:
        # print("PDF read error:", e)
        return ""

# Build the DataFrame df
rows = []
skipped = []

all_files = [p for p in base.rglob("*") if p.is_file()]

for p in all_files:
    ext = p.suffix.lower()
    if ext not in [".pdf", ".docx"]:
        continue  # ignore other file types

    # folder name is label (e.g. 'DataScience', 'HR')
    label = p.parts[-2]

    if ext == ".docx":
        text = read_docx(str(p))
    elif ext == ".pdf":
        text = read_pdf(str(p))
    else:
        text = ""

    if text.strip():
        rows.append({
            "filename": p.name,
            "filepath": str(p),
            "label": label,
            "text": text
        })
    else:
        skipped.append(str(p))

df = pd.DataFrame(rows)

print(" DataFrame 'df' CREATED SUCCESSFULLY ")
print("Total Documents:", len(df))
print("Skipped Files:", len(skipped))
print("Columns:", df.columns.tolist())
df.head()

# EDA – Basic Exploratory Analysis
print("Columns:", df.columns.tolist())
print("\nLabel distribution:")
print(df['label'].value_counts())

plt.figure(figsize=(8,4))
df['label'].value_counts().plot(kind='bar')
plt.title("Number of resumes per class")
plt.xlabel("Class")
plt.ylabel("Count")
plt.show()

# Text length stats
df['char_count'] = df['text'].apply(len)
df['word_count_raw'] = df['text'].apply(lambda x: len(str(x).split()))

print(df[['char_count', 'word_count_raw']].describe())

plt.figure(figsize=(8,4))
df['word_count_raw'].hist(bins=30)
plt.title("Distribution of word counts in resumes")
plt.xlabel("Word count")
plt.ylabel("Frequency")
plt.show()

# Quick sample
df[['filename', 'label']].head(10)

# one full resume text sample:
sample_row = df.iloc[0]
print("Filename:", sample_row['filename'])
print("Label:", sample_row['label'])
print("--- Text snippet ---")
print(sample_row['text'][:1000])

# Text cleaning functions
# Use the same cleaning strategy for training and Streamlit prediction.
def clean_text_basic(s):
    s = str(s).lower()
    s = re.sub(r'\s+', ' ', s)            # collapse whitespace
    s = re.sub(r'http\S+', '', s)         # remove urls
    s = re.sub(r'\@\w+', '', s)           # remove @mentions
    s = re.sub(r'[^a-z0-9\s]', ' ', s)    # keep alphanumeric + space
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def remove_stopwords(s):
    tokens = s.split()
    tokens = [t for t in tokens if t not in stop_words]
    return " ".join(tokens)

df['clean_text'] = df['text'].apply(clean_text_basic)
df['clean_text_nostop'] = df['clean_text'].apply(remove_stopwords)

df['char_count_clean'] = df['clean_text'].apply(len)
df['word_count_clean'] = df['clean_text'].apply(lambda x: len(x.split()))

print(df[['char_count_clean', 'word_count_clean']].describe())

# Train/test split
X = df['clean_text']       # or 'clean_text_nostop'
y = df['label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

print("Train size:", len(X_train))
print("Test size:", len(X_test))

# Train & evaluate multiple models
def train_and_eval(clf, X_train, y_train, X_test, y_test, name=None):
    pipe = Pipeline([
        ('tfidf', TfidfVectorizer(
            ngram_range=(1,2),
            max_df=0.95,
            min_df=2,
            max_features=15000
        )),
        ('clf', clf)
    ])

    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)

    acc = accuracy_score(y_test, preds)
    f1 = f1_score(y_test, preds, average='macro')

    print(f"\n--- {name or clf.__class__.__name__} ---")
    print("Accuracy :", acc)
    print("Macro F1 :", f1)
    print(classification_report(y_test, preds))

    return pipe, acc, f1, preds

models = {
    'LogisticRegression': LogisticRegression(max_iter=2000),
    'MultinomialNB': MultinomialNB(),
    'LinearSVC': LinearSVC(max_iter=20000),
    'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42)
}

results = []
pipelines = {}

for name, clf in models.items():
    pipe, acc, f1, preds = train_and_eval(clf, X_train, y_train, X_test, y_test, name=name)
    results.append({'model': name, 'accuracy': acc, 'f1_macro': f1})
    pipelines[name] = pipe

res_df = pd.DataFrame(results).sort_values('f1_macro', ascending=False).reset_index(drop=True)
res_df

# Plot comparison + confusion matrix for best model
plt.figure(figsize=(8,4))
plt.bar(res_df['model'], res_df['accuracy'])
plt.title('Model accuracy comparison')
plt.ylim(0,1)
plt.show()

plt.figure(figsize=(8,4))
plt.bar(res_df['model'], res_df['f1_macro'])
plt.title('Model macro F1 comparison')
plt.ylim(0,1)
plt.show()

# Confusion matrix:
best_model_name = res_df.loc[0, 'model']
best_pipe = pipelines[best_model_name]

y_pred = best_pipe.predict(X_test)
cm = confusion_matrix(y_test, y_pred, labels=best_pipe.classes_)

fig, ax = plt.subplots(figsize=(8,6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=best_pipe.classes_,
            yticklabels=best_pipe.classes_)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title(f'Confusion Matrix - {best_model_name}')
plt.show()

# Save the best model for deployment
pickle_filename = 'best_model.pkl'
joblib.dump(best_pipe, pickle_filename)
print(f"The best model ('{best_model_name}') has been saved to '{pickle_filename}'")

